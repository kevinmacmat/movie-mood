{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import urllib\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDb Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_akas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_akas = pd.read_csv('title.akas.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.iloc[11000000:11000020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.loc[(df_akas['titleId']=='tt6105098')&(df_akas['region']=='US')]\n",
    "# df_akas.loc[(df_akas['titleId']=='tt4154796')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.loc[(df_akas['title']=='The Sixth Sense')&(df_akas['region']=='US')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas[df_akas['isOriginalTitle']==1]['language'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.attributes.iloc[100:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.language.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas.region.value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_akas[df_akas['title'] == 'The Wind'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics = pd.read_csv('title.basics.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_basics.iloc[1500000:1500010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('title.ratings.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Office Mojo Worldwide DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.boxofficemojo.com/year/world/'\n",
    "def get_worldwide(url):\n",
    "    '''\n",
    "    take in a url and obtain a list of urls to parse through\n",
    "    '''\n",
    "    #container for holding dataframe\n",
    "    container = []\n",
    "    \n",
    "    #for each year between 1999, 2019, create a url to easily read the html and output a dataframe of top 100 in year\n",
    "    for year in range(1999,2020): #1999, 2019   \n",
    "        df = pd.read_html(url + str(year))       \n",
    "        container.append(df[0][:100]) \n",
    "    #concat the list of dataframes and return one big dataframe\n",
    "    return pd.concat(container, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Box Office Mojo - Foreign Markets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_worldwide_link_list(url):\n",
    "    '''\n",
    "    Given the url to a year, obtain a list of working urls to later parse through\n",
    "    '''\n",
    "    container = []\n",
    "    for year in range(2019,1998,-1): #1999, 2019\n",
    "        url + str(year)\n",
    "        container.append(url + str(year))\n",
    "    return container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_hyperlinks(link_list):    #give it a list\n",
    "    '''\n",
    "    Given a list of year urls, go into the hyperlink of each table row (title of movie),\n",
    "    Retrieve a long list of hyperlinks containing country revenue information\n",
    "    '''\n",
    "    link_results = []\n",
    "    for link in link_list:   #use one entry for ex. 2019\n",
    "        page = requests.get(link) #requests 2019 url\n",
    "        src = page.content  #turn to page\n",
    "        soup = BeautifulSoup(src, 'html.parser') #turn 2019 to soup\n",
    "        all_a = soup.find_all('a') #grab anchors from 2019 soup an put into a list of links\n",
    "        c = []  \n",
    "        for x in all_a: #looking through hyper links 'hrefs'\n",
    "            c.append(x.attrs['href']) #append link to container\n",
    "            # cleans hyperlinks to only include relevant\n",
    "        movie_urls_year = []\n",
    "        for x in c[:119]:         #filters through slice of container to get wanted releasegroup urls\n",
    "            if 'releasegroup' in x:   \n",
    "                movie_urls_year.append(x)\n",
    "        link_results.extend(movie_urls_year)\n",
    "    return link_results\n",
    "\n",
    "# get_country_hyperlinks(link_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countries_for_title(url):\n",
    "    '''\n",
    "    For given url, go to site and obtain foriegn countries revenue if possible\n",
    "    '''\n",
    "    # Make a request for each title\n",
    "    html_table = urllib.request.urlopen('https://www.boxofficemojo.com/{}'.format(url)).read()\n",
    "\n",
    "    # Turn into soup\n",
    "    soop = BeautifulSoup(html_table, \"html.parser\")\n",
    "    \n",
    "    # Grab title\n",
    "    title = soop.find('h1', class_='a-size-extra-large').get_text()\n",
    "    \n",
    "    # Grab countries\n",
    "    for table in soop.findChildren(attrs={'class': 'a-align-center'}):  #wat this do again\n",
    "        for c in table.children:\n",
    "            if c.name in ['tr', 'th']:\n",
    "                c.unwrap()\n",
    "\n",
    "    #obtain a list of dataframes from reading html, this will contain domestic information\n",
    "    df_market = pd.read_html(str(soop), flavor=\"bs4\") #list of data\n",
    "    \n",
    "    #foriegn information will exist in lists with length > 1 \n",
    "    if len(df_market) > 1:\n",
    "        list_dataframe = [] #[df1,df2,df3]   [df1,]\n",
    "        for dataframe in df_market[1:]:  #take foriegn information\n",
    "            list_dataframe.append(dataframe.droplevel(level=0,axis=1))   #fix column's multi-index\n",
    "        result = pd.concat(list_dataframe,ignore_index = True)    #concat columnwise \n",
    "        result['title'] = title  #create new column with repeating title\n",
    "        return result\n",
    "    else:\n",
    "    #if len is 1, then return empty dataframe with columns\n",
    "        result = pd.DataFrame(columns = ['Market', 'Release Date', 'Opening', 'Gross', 'title'])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_values(result, result_2):\n",
    "    ''' propogate result_2 dataframe's columns throughout result dataframe.\n",
    "        result_2 will have max of 4 columns with set column names\n",
    "    '''\n",
    "    a1 = result_2.values[0][0]\n",
    "    a2 = result_2.values[0][1]\n",
    "    a3 = result_2.values[0][2]\n",
    "    a4 = result_2.values[0][3]\n",
    "    \n",
    "    act_dict = {\n",
    "        'Actor 1' : a1,\n",
    "        'Actor 2' : a2,\n",
    "        'Actor 3' : a3,\n",
    "        'Actor 4' : a4\n",
    "    }\n",
    "    #concat and fill the rest of NaN with values from result 2\n",
    "    return pd.concat([result, result_2],axis = 1).fillna(value = act_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_actors(url):\n",
    "    '''Given url from clicking 'cast and crew' button, take in information and return a df with actors'''\n",
    "    result = []\n",
    "    \n",
    "    request_url = url\n",
    "    response = requests.get(request_url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    inner_container = []\n",
    "    \n",
    "    for entry in soup.find('table', id = 'principalCast').findAll('tr')[1:]:\n",
    "        #clean and obtain only the actor's name from [actor,char]\n",
    "        inner_container.append(entry.get_text()[:-8].split('\\n\\n')[0])  \n",
    "    result.append(inner_container)\n",
    "    \n",
    "    columns = ['Actor 1','Actor 2','Actor 3','Actor 4']\n",
    "    \n",
    "    #if the length of Actors is less than 4, fill the remaining with NaN\n",
    "    while len(inner_container) < 4:\n",
    "        inner_container.append(np.nan)\n",
    "    \n",
    "    actor_frame = pd.DataFrame(result,columns = columns)\n",
    "    return actor_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reroute_twice(url):\n",
    "    '''\n",
    "    get from release link -> title release -> cast and crew\n",
    "    '''\n",
    "    home = 'https://www.boxofficemojo.com'\n",
    "    \n",
    "    #first click\n",
    "    request_url = home + url\n",
    "    response = requests.get(request_url)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    appended = soup.find('a','a-link-normal mojo-title-link refiner-display-highlight').get('href')\n",
    "\n",
    "    second = '{}{}'.format(home,appended) #second url\n",
    "    #get the second click\n",
    "    response2 = requests.get(second)\n",
    "    soup2 = BeautifulSoup(response2.content)\n",
    "    appended2 = soup2.find('a',class_='a-size-base a-link-normal mojo-navigation-tab').get('href')\n",
    "    final_url = home + appended2\n",
    "    return final_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_save(dataframe, csv_filename):\n",
    "    '''save final dataframe into existing csv'''\n",
    "    f = open(csv_filename)\n",
    "    columns = ['Market', 'Release Date', 'Opening', 'Gross', 'title', 'Actor 1','Actor 2', 'Actor 3', 'Actor 4']\n",
    "    \n",
    "    #make existing dataframe to append\n",
    "    data = pd.read_csv(f)\n",
    "    df1 = pd.DataFrame(data,columns=columns)\n",
    "    \n",
    "    #make input dataframe into dataframe\n",
    "    df2 = pd.DataFrame(dataframe, columns = columns)\n",
    "    \n",
    "    df3 = pd.concat([df1,df2])\n",
    "    \n",
    "    #overwrites df to csv\n",
    "    df3.to_csv('movies_test.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_or_clear_movies_csv(csv_filename):\n",
    "    ''' make a csv if it doesnt exist, else clear the existing one leaving headers'''\n",
    "\n",
    "    headers = pd.DataFrame(columns = ['Market', 'Release Date', 'Opening', 'Gross', 'title', 'Actor 1',\n",
    "       'Actor 2', 'Actor 3', 'Actor 4'])\n",
    "    \n",
    "    if os.path.exists(csv_filename):\n",
    "        #clear it\n",
    "        f = open(csv_filename, 'w')\n",
    "        f.truncate() #eviscerated\n",
    "        headers.to_csv(csv_filename, mode='a')\n",
    "    else:\n",
    "        headers.to_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_list = get_worldwide_link_list(url) #lengthy load\n",
    "country_list = get_country_hyperlinks(link_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_or_clear_movies_csv('movies_test.csv') #check for existing csv, if doesnt exist, make one, else clear it out\n",
    "url = 'https://www.boxofficemojo.com/year/world/'\n",
    "\n",
    "for country in country_list:  #country_list #get 1 title's hyperlink for all countries\n",
    "\n",
    "    df1 = pd.DataFrame(get_countries_for_title(country)) #returns first dataframe with country data\n",
    "\n",
    "    url2 = reroute_twice(country) #get to cast hyperlink\n",
    "    \n",
    "    df2 = pd.DataFrame(scrape_actors(url2)) #scrap actors and return second dataframe\n",
    "\n",
    "    if not df1.empty:  #If the first df is not empty then fill values and concat -> save\n",
    "        final_df = fill_values(df1, df2) #Fills NaN values with actor names and concatenates\n",
    "        result = pd.DataFrame(final_df)\n",
    "        data_save(result,'movies_test.csv')\n",
    "        print('saving')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum worldwide and domestic and take mean\n",
    "#avg revenue in anad out of US\n",
    "#popular actors in specific countries?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #/releasegroup/gr3511898629/?ref_=bo_ydw_table_1 Change with list\n",
    "# html_table = urllib.request.urlopen('https://www.boxofficemojo.com/releasegroup/gr3511898629/?ref_=bo_ydw_table_1').read()\n",
    "\n",
    "# # fix HTML\n",
    "# soop = BeautifulSoup(html_table, \"html.parser\")\n",
    "# for table in soup.findChildren(attrs={'class': 'a-align-center'}): \n",
    "#     for c in table.children:\n",
    "#         if c.name in ['tr', 'th']:\n",
    "#             c.unwrap()\n",
    "\n",
    "# df_market = pd.read_html(str(soop), flavor=\"bs4\")\n",
    "# df_market_1 = df_market[1]\n",
    "# df_market_2 = df_market[2]\n",
    "# df_market_3 = df_market[3]\n",
    "# df_market_4 = df_market[4]\n",
    "# new_market = pd.DataFrame(df_market[1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
